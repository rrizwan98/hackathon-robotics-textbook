---
id: 0009
title: Implement Gemini File API for Embeddings
stage: green
date: 2025-12-02
surface: agent
model: gemini
feature: 001-textbook-ai-portal
branch: feat/setup-docusaurus-frontend
user: user
command: /sp.implement
labels: [gemini, file-api, embeddings, backend, script, test]
links:
  spec: specs/001-textbook-ai-portal/spec.md
  ticket: null
  adr: null
  pr: https://github.com/rrizwan98/hackathon-robotics-textbook/pull/2
files:
- backend/requirements.txt
- backend/src/scripts/embed_textbook_gemini.py
- backend/src/scripts/upload_textbook_files_gemini.py
- backend/tests/test_gemini_file_api.py
- backend/data/gemini_files.json
tests:
- backend/tests/test_gemini_file_api.py
---

## Prompt

/sp.implement now let's build the step # 4. here is the detail. IMPLEMENTATION STEP 4 ONLY ‚Äî UPDATED EMBEDDING FLOW (GEMINI FILE API).

Context:
- We are using Gemini embedding model: "gemini-embedding-001".
- We will NOT use an external vector store (e.g. Qdrant) for now.
- Embeddings will be used later directly (or cached locally) as needed.
- We only need a minimal test to verify embedding correctness (not full RAG pipeline yet).

Tasks:

1. Create an embedding script inside `backend/src/`, e.g.:
   `scripts/embed_textbook_gemini.py`

2. The script should:
   a. Load all Markdown files from the book docs directory (`frontend/site/docs/`) ‚Äî or wherever the final book markdown lives.
   b. For each file:
      - Extract text content (e.g. full text, or split into chunks if desired).
      - (Optional) Attach metadata: file path, chapter, section.

3. Use Gemini embedding model `"gemini-embedding-001"` to compute embeddings for each text (or chunk).  
   - Use your configured Gemini credentials / API keys.  
   - Ensure consistency: same model, same parameters.

4. Store embeddings temporarily (e.g. in a local JSON or pickle file) ‚Äî purely for testing; **do not persist to any external DB**.  
   Example file: `backend/data/embeddings.json`  
   The structure can be:
   ```json
   {
     "path/to/chapter1.md": embedding_vector_1,
     "path/to/chapter2.md": embedding_vector_2,
     ...
   }
I'ready saved the gemini api key in .env file in root you can move it to backend folder. or any where where you can use

## Response snapshot

Summary of changes for Task 4 (UPDATED EMBEDDING FLOW):
*   Recreated necessary backend directory structure.
*   Created `backend/src/scripts/upload_textbook_files_gemini.py` to upload specific Markdown files to Gemini File API.
*   Created `backend/tests/test_gemini_file_api.py` to verify QA functionality against uploaded files.
*   Updated `backend/requirements.txt` with `pytest` and ensured `google-generativeai` is updated.
*   Successfully uploaded 'intro.md' and 'ros2.md' to Gemini File API and saved references.
*   Successfully ran the QA test against an uploaded file, confirming non-empty responses.

## Outcome

- ‚úÖ Impact: Successfully implemented and tested the Gemini File API flow for uploading textbook content and running QA queries.
- üß™ Tests: pytest backend/tests/test_gemini_file_api.py passed.
- üìÅ Files: Created backend scripts and tests for Gemini File API. Modified previous embedding script. Generated gemini_files.json.
- üîÅ Next prompts: Proceed with next implementation step or integrate with other components.
- üß† Reflection: Encountered several issues with the google-generativeai library's API for File Upload and Client instantiation, requiring multiple iterations and debugging with context7 to find the correct usage. Also faced Gemini API quota limits, leading to a revised plan for processing fewer files.

## Evaluation notes (flywheel)

- Failure modes observed: Incorrect API usage, Gemini API quota limits.
- Graders run and results (PASS/FAIL): PASS
- Prompt variant (if applicable): null
- Next experiment (smallest change to try): null
